{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ebe75c-08a6-4b3c-9eb8-344dc9598aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import gp\n",
    "import operator\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049b724-830e-4644-8e10-b9f9b57c7b24",
   "metadata": {},
   "source": [
    "## Primitive set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a9136f-e70a-4c70-a512-9d3c837c0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def protectedDiv(left, right):\n",
    "    try:\n",
    "        return left / right\n",
    "    except ZeroDivisionError:\n",
    "        return 1\n",
    "\n",
    "pset = gp.PrimitiveSet(\"MAIN\", 1)\n",
    "pset.addPrimitive(operator.add, 2)\n",
    "pset.addPrimitive(operator.sub, 2)\n",
    "pset.addPrimitive(operator.mul, 2)\n",
    "pset.addPrimitive(protectedDiv, 2)\n",
    "pset.addPrimitive(math.cos, 1)\n",
    "\n",
    "pset.renameArguments(ARG0='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e614af-f902-4c39-8494-7476a04e9f0b",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9970165-e9d1-493f-b56a-66c07d628049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_tree(pset, min_, max_):\n",
    "    expr = gp.genHalfAndHalf(pset, min_=min_, max_=max_)\n",
    "    tree = gp.PrimitiveTree(expr)\n",
    "    return tree\n",
    "\n",
    "def build_primitives_terminals_dict(pset):\n",
    "    prims = dict()\n",
    "    prims_funcs = list(pset.primitives.values())[0]\n",
    "    prims_names = [p.name for p in prims_funcs]\n",
    "    prims.update(zip(prims_names, prims_funcs))\n",
    "\n",
    "    # for arguments, add key = value = name to the dict\n",
    "    for arg in pset.arguments:\n",
    "        prims[str(arg)] = arg\n",
    "\n",
    "    return prims\n",
    "\n",
    "def tree_to_nodes_matrix(tree: gp.PrimitiveTree, pset: gp.PrimitiveSet, prims_names: list, n_nodes=0):\n",
    "    n_prims = pset.prims_count + len(pset.arguments)\n",
    "    if n_nodes == 0:\n",
    "        n_nodes = len(tree)\n",
    "    m = np.zeros((n_nodes, n_prims))\n",
    "\n",
    "    for i, prim in enumerate(tree):\n",
    "        prim_name = prim.name.replace('ARG0', 'x')\n",
    "        prim_idx = prims_names.index(prim_name)\n",
    "        m[i, prim_idx] = 1.\n",
    "    \n",
    "    return m\n",
    "\n",
    "def eval_fitness(tree, pset, points):\n",
    "    func = gp.compile(tree, pset)\n",
    "\n",
    "    sqerrors = ((func(x) - (x**2 + math.cos(x)))**2 for x in points)\n",
    "    return math.fsum(sqerrors) / len(points)\n",
    "\n",
    "def generate_dataset(n_samples, pset, min_, max_, points, prims_names):\n",
    "    n_prims = pset.prims_count + len(pset.arguments)\n",
    "    max_nodes = 2**(max_+1) - 1\n",
    "    X = np.zeros((n_samples, max_nodes*n_prims))\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        tree = generate_random_tree(pset, min_, max_)\n",
    "        m = tree_to_nodes_matrix(tree, pset, prims_names, max_nodes).ravel()\n",
    "        X[i,:] = m\n",
    "        fit = eval_fitness(tree, pset, points)\n",
    "        if math.isnan(fit) or math.isinf(fit):\n",
    "            fit = 1e2\n",
    "        y[i,:] = fit\n",
    "\n",
    "    return X, y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97c4d3-be8c-47e7-aa55-3f7388a9993e",
   "metadata": {},
   "source": [
    "## Generation of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c67ec91-c720-41c8-804e-24de7a927563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add(x, x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = generate_random_tree(pset, min_=1, max_=3)\n",
    "print(tree)\n",
    "prims = build_primitives_terminals_dict(pset)\n",
    "tree_to_nodes_matrix(tree, pset, list(prims.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5251f4-9fb4-4db7-976b-2c18c4f380bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2512503477471599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = np.arange(0.,1.1,0.1)\n",
    "print(points)\n",
    "eval_fitness(tree, pset, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b152ea-57ba-4813-9f78-865eb14f7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f59060-6001-4a0d-afb0-2071ea14dd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.8128734]), array([1.25222875]), array([-0.81672707]), array([-0.79874873]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81330714]), array([-0.80318462]), array([-0.80787818]), array([-0.8128734]), array([-0.80318462]), array([-0.81330714]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.81265794]), array([-0.78839083]), array([-0.81330714]), array([1.25222875]), array([-0.8128734]), array([-0.81330714]), array([-0.80318462]), array([1.25222875]), array([-0.81258848]), array([1.25222875]), array([1.25222875]), array([-0.77938746]), array([1.25222875]), array([-0.81417083]), array([-0.81672707]), array([1.25222875]), array([-0.80318462]), array([-0.81417083]), array([-0.81131713]), array([1.25222875]), array([-0.80318462]), array([-0.81330714]), array([-0.81330714]), array([-0.78839083]), array([-0.81672707]), array([-0.80318462]), array([1.25222875]), array([-0.79900341]), array([1.25222875]), array([-0.80048557]), array([-0.80318462]), array([-0.8123586]), array([-0.78839083]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.80513322]), array([-0.80318462]), array([-0.80709622]), array([-0.81330714]), array([-0.81330714]), array([-0.81330714]), array([-0.8128734]), array([-0.81330714]), array([-0.78839083]), array([-0.78839083]), array([-0.78839083]), array([-0.80318462]), array([-0.78839083]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([-0.80318462]), array([-0.8128734]), array([-0.78839083]), array([-0.81330714]), array([1.25222875]), array([-0.8123586]), array([1.25222875]), array([-0.81744774]), array([1.25222875]), array([-0.78839083]), array([-0.79765291]), array([-0.81330714]), array([-0.80154685]), array([1.25222875]), array([1.25222875]), array([-0.73508521]), array([1.25222875]), array([-0.8128734]), array([-0.80318462]), array([-0.8112853]), array([-0.78839083]), array([-0.78839083]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.79208452]), array([-0.81330714]), array([1.25222875]), array([-0.81330714]), array([-0.80844259]), array([-0.80411198]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([-0.81330714]), array([1.25222875]), array([-0.8128734]), array([-0.81451995]), array([1.25222875]), array([-0.81744774]), array([-0.80513322]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([-0.81330714]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.70464756]), array([-0.78839083]), array([-0.79765291]), array([-0.80318462]), array([-0.81672707]), array([-0.8128734]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([-0.79765291]), array([1.25222875]), array([-0.78749944]), array([-0.81330714]), array([-0.78839083]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([-0.81807505]), array([-0.81330714]), array([1.25222875]), array([-0.81330714]), array([1.25222875]), array([-0.8128734]), array([-0.8123586]), array([-0.81417083]), array([1.25222875]), array([-0.78839083]), array([-0.81330714]), array([-0.81672707]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81330714]), array([-0.8128734]), array([1.25222875]), array([-0.80513322]), array([-0.78839083]), array([1.25222875]), array([-0.76872524]), array([-0.78839083]), array([-0.78839083]), array([-0.81761301]), array([-0.81330714]), array([1.25222875]), array([-0.8128734]), array([-0.80844259]), array([-0.78886973]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([-0.80513322]), array([-0.81330714]), array([-0.80768879]), array([-0.81589998]), array([-0.81330714]), array([-0.79208724]), array([1.25222875]), array([-0.78839083]), array([-0.8128734]), array([-0.79035108]), array([1.25222875]), array([-0.81673146]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.7996356]), array([-0.81417083]), array([-0.79035108]), array([-0.80318462]), array([1.25222875]), array([-0.80318462]), array([-0.76154954]), array([1.25222875]), array([-0.81330714]), array([1.25222875]), array([-0.81330714]), array([1.25222875]), array([-0.81350895]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.81672707]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.7946744]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.8123586]), array([-0.8128734]), array([-0.80597207]), array([1.25222875]), array([-0.78839083]), array([-0.74863457]), array([-0.78839083]), array([1.25222875]), array([-0.80318462]), array([-0.81330714]), array([-0.8102062]), array([-0.78966292]), array([1.25222875]), array([-0.79874873]), array([1.25222875]), array([-0.78839083]), array([-0.81330714]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([-0.73801935]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([-0.8128734]), array([-0.8128734]), array([-0.8088431]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([-0.63385748]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([-0.71869508]), array([-0.80119503]), array([1.25222875]), array([-0.7889197]), array([-0.80318462]), array([-0.78839083]), array([-0.81330714]), array([-0.81330714]), array([1.25222875]), array([-0.81330714]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.8123586]), array([-0.74350689]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([-0.78839083]), array([-0.79765291]), array([1.25222875]), array([-0.8128734]), array([-0.80318462]), array([-0.76657584]), array([-0.81265794]), array([1.25222875]), array([-0.8128734]), array([-0.80318462]), array([-0.81330714]), array([1.25222875]), array([1.25222875]), array([-0.81330714]), array([-0.80048557]), array([1.25222875]), array([-0.80562846]), array([-0.7588389]), array([-0.78839083]), array([1.25222875]), array([-0.79765291]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([-0.78839083]), array([-0.81330714]), array([1.25222875]), array([-0.8123586]), array([-0.78839083]), array([-0.81330714]), array([-0.79965395]), array([1.25222875]), array([-0.81330714]), array([1.25222875]), array([-0.81330714]), array([-0.80709622]), array([-0.80318462]), array([1.25222875]), array([-0.79765291]), array([-0.78839083]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.76686842]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.78886973]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81330714]), array([-0.81330714]), array([1.25222875]), array([0.29044166]), array([-0.78839083]), array([-0.8128734]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([-0.8128734]), array([-0.76406122]), array([1.25222875]), array([1.25222875]), array([-0.81330714]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80562846]), array([1.25222875]), array([-0.70593976]), array([-0.76154954]), array([1.25222875]), array([-0.78839083]), array([-0.78839083]), array([-0.80285051]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.81330714]), array([-0.8128734]), array([-0.81330714]), array([1.25222875]), array([-0.8128734]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.81350895]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([-0.76971658]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([-0.80329943]), array([-0.81417083]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([-0.78839083]), array([-0.81330714]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([-0.80783354]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([-0.78839083]), array([-0.81330714]), array([1.25222875]), array([-0.80318462]), array([-0.78041907]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([-0.59916098]), array([-0.80318462]), array([-0.8128734]), array([-0.80318462]), array([-0.64994592]), array([-0.80318462]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([-0.8128734]), array([-0.81330714]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([-0.81330714]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([-0.8128734]), array([-0.78839083]), array([1.25222875]), array([-0.78839083]), array([-0.80318462]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.80709622]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([-0.81330714]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([-0.79965395]), array([-0.80318462]), array([-0.70593976]), array([-0.80690461]), array([-0.78839083]), array([-0.81330714]), array([1.25222875]), array([-0.80489385]), array([1.25222875]), array([-0.8123586]), array([1.25222875]), array([-0.8088431]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80337649]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.79853903]), array([1.25222875]), array([-0.80844259]), array([1.25222875]), array([-0.79765291]), array([-0.81330714]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([-0.66999434]), array([-0.78839083]), array([-0.78839083]), array([-0.81672707]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.8123586]), array([1.25222875]), array([-0.81330714]), array([-0.81330714]), array([1.25222875]), array([-0.7457097]), array([1.25222875]), array([-0.81350895]), array([1.25222875]), array([-0.78839083]), array([-0.81330714]), array([-0.81417083]), array([1.25222875]), array([-0.79909889]), array([-0.78886973]), array([-0.81330714]), array([-0.78839083]), array([1.25222875]), array([-0.8123586]), array([-0.81330714]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([-0.8128734]), array([-0.81330714]), array([-0.78839083]), array([-0.79493524]), array([1.25222875]), array([-0.79765291]), array([-0.81330714]), array([1.25222875]), array([-0.76549451]), array([1.25222875]), array([-0.8100163]), array([-0.80787818]), array([-0.81330714]), array([-0.80318462]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.79696175]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81589998]), array([1.25222875]), array([-0.80318462]), array([-0.80844259]), array([1.25222875]), array([-0.8100163]), array([-0.80329943]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([-0.80318462]), array([-0.81417083]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([-0.80513322]), array([-0.80318462]), array([-0.8128734]), array([-0.75851335]), array([-0.81131713]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([-0.8128734]), array([-0.8128734]), array([-0.81672707]), array([1.25222875]), array([-0.79965395]), array([-0.80337649]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([-0.8128734]), array([-0.81330714]), array([-0.73369368]), array([-0.8128734]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([-0.80318462]), array([-0.81672707]), array([-0.8128734]), array([-0.8128734]), array([1.25222875]), array([-0.8128734]), array([-0.80318462]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81330714]), array([-0.8128734]), array([-0.8088431]), array([1.25222875]), array([-0.81744774]), array([-0.8128734]), array([1.25222875]), array([-0.76406122]), array([-0.69192983]), array([-0.8128734]), array([-0.80318462]), array([-0.80337649]), array([-0.78839083]), array([1.25222875]), array([-0.80844259]), array([-0.8128734]), array([-0.80318462]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([-0.78839083]), array([-0.81330714]), array([-0.80844259]), array([-0.78015838]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80844259]), array([-0.8128734]), array([-0.78839083]), array([-0.76154954]), array([1.25222875]), array([-0.79900341]), array([1.25222875]), array([-0.78342657]), array([-0.80318462]), array([1.25222875]), array([-0.80318462]), array([-0.78400285]), array([-0.81417083]), array([-0.8128734]), array([-0.80197807]), array([1.25222875]), array([-0.73612939]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([-0.79965395]), array([-0.79765291]), array([-0.81330714]), array([-0.8128734]), array([-0.81330714]), array([-0.78839083]), array([-0.81131713]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([-0.74873385]), array([-0.79965395]), array([-0.8128734]), array([-0.80318462]), array([-0.8128734]), array([-0.79765291]), array([-0.8123586]), array([1.25222875]), array([-0.80318462]), array([-0.78839083]), array([-0.79898811]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([-0.80096072]), array([-0.81702873]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80513322]), array([-0.80787818]), array([-0.76852777]), array([-0.78543121]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81417083]), array([-0.8128734]), array([-0.8128734]), array([-0.80787818]), array([-0.8128734]), array([-0.78059182]), array([1.25222875]), array([-0.80787818]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81417083]), array([-0.8100163]), array([-0.81807505]), array([-0.78839083]), array([-0.74606622]), array([1.25222875]), array([-0.8128734]), array([-0.80318462]), array([-0.80318462]), array([-0.8123586]), array([1.25222875]), array([-0.78839083]), array([-0.79965395]), array([-0.78839083]), array([1.25222875]), array([-0.78909079]), array([-0.81417083]), array([-0.81330714]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([-0.81417083]), array([-0.78365692]), array([-0.80197807]), array([1.25222875]), array([-0.76154954]), array([1.25222875]), array([-0.79874873]), array([-0.80787818]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([-0.78839083]), array([-0.80318462]), array([-0.7737972]), array([-0.8128734]), array([-0.70593976]), array([-0.81330714]), array([-0.7946744]), array([-0.8128734]), array([-0.81417083]), array([1.25222875]), array([-0.81330714]), array([-0.81330714]), array([-0.81330714]), array([-0.77035413]), array([1.25222875]), array([-0.8128734]), array([-0.80318462]), array([-0.8128734]), array([-0.78839083]), array([-0.80318462]), array([-0.8128734]), array([-0.8128734]), array([-0.78839083]), array([-0.81672707]), array([-0.80428498]), array([-0.81330714]), array([-0.8123586]), array([-0.81330714]), array([-0.80513322]), array([1.25222875]), array([-0.8128734]), array([-0.8123586]), array([-0.80513322]), array([-0.80083713]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81807505]), array([-0.80489385]), array([1.25222875]), array([-0.81330714]), array([-0.80318462]), array([1.25222875]), array([-0.80489385]), array([-0.78839083]), array([1.25222875]), array([-0.81672707]), array([-0.80318462]), array([1.25222875]), array([-0.80318462]), array([-0.79035108]), array([-0.80318462]), array([-0.81330714]), array([-0.81417083]), array([-0.81330714]), array([1.25222875]), array([-0.81330714]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81330714]), array([-0.81330714]), array([1.25222875]), array([-0.8128734]), array([-0.76406122]), array([-0.78839083]), array([-0.81015909]), array([-0.80787818]), array([-0.78839083]), array([-0.79325804]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81672707]), array([-0.80318462]), array([-0.8128734]), array([-0.81330714]), array([-0.78839083]), array([-0.78839083]), array([-0.79765291]), array([1.25222875]), array([-0.8128734]), array([-0.78839083]), array([-0.8128734]), array([1.25222875]), array([-0.8128734]), array([-0.8128734]), array([-0.80411198]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([-0.79765291]), array([1.25222875]), array([-0.81672707]), array([1.25222875]), array([-0.81330714]), array([-0.80489385]), array([1.25222875]), array([-0.75851335]), array([1.25222875]), array([1.25222875]), array([-0.79765291]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([-0.79853903]), array([-0.80411198]), array([1.25222875]), array([-0.80489385]), array([1.25222875]), array([-0.78839083]), array([-0.80569331]), array([-0.8128734]), array([1.25222875]), array([-0.8128734]), array([-0.80318462]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([-0.81807505]), array([-0.78839083]), array([-0.78839083]), array([-0.78839083]), array([-0.8128734]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([-0.78886973]), array([1.25222875]), array([-0.78839083]), array([-0.81330714]), array([-0.80787818]), array([-0.69168658]), array([-0.78383377]), array([1.25222875]), array([-0.80318462]), array([-0.78839083]), array([-0.81330714]), array([-0.78839083]), array([1.25222875]), array([-0.81170337]), array([-0.81330714]), array([-0.78884797]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([-0.81330714]), array([-0.8128734]), array([-0.8123586]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([-0.8123586]), array([1.25222875]), array([-0.79853903]), array([1.25222875]), array([-0.79765291]), array([1.25222875]), array([-0.80318462]), array([-0.79765291]), array([-0.75851335]), array([1.25222875]), array([-0.78886973]), array([1.25222875]), array([-0.80318462]), array([-0.8128734]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([-0.81633068]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.80318462]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([-0.81330714]), array([1.25222875]), array([-0.377442]), array([-0.78839083]), array([-0.78839083]), array([1.25222875]), array([1.25222875]), array([-0.78839083]), array([-0.80318462]), array([-0.78839083]), array([-0.76406122]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.8128734]), array([1.25222875]), array([-0.81551138]), array([-0.80661218]), array([-0.76406122]), array([1.25222875]), array([-0.81672707]), array([-0.78839083]), array([-0.81330714]), array([1.25222875]), array([-0.77938746]), array([-0.80709622]), array([-0.81330714]), array([-0.81330714]), array([-0.80318462]), array([1.25222875]), array([-0.81417083]), array([1.25222875]), array([-0.80318462]), array([-0.74350689]), array([-0.80318462]), array([-0.8128734]), array([-0.81330714]), array([1.25222875]), array([-0.78839083]), array([-0.81330714]), array([-0.81330714]), array([1.25222875]), array([1.25222875]), array([1.25222875]), array([-0.81672707])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2483/2793393697.py:3: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return left / right\n",
      "/tmp/ipykernel_2483/2793393697.py:3: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return left / right\n",
      "<string>:1: RuntimeWarning: invalid value encountered in scalar multiply\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "min_ = 1\n",
    "max_ = 3\n",
    "max_nodes = 2**(max_+1) - 1\n",
    "n_prims = n_prims = pset.prims_count + len(pset.arguments)\n",
    "n_samples = 1000\n",
    "X, y = generate_dataset(n_samples, pset, min_, max_, points, list(prims.keys()))\n",
    "y_normalized = (y - np.mean(y))/np.std(y)\n",
    "frac = 0.8\n",
    "X_train, X_valid = random_split(X, [frac, 1-frac])\n",
    "y_train, y_valid = random_split(y_normalized, [frac, 1-frac])\n",
    "print(list(y_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba21cbe1-d707-4cdc-9e72-d0b240b05e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None, target_transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return X[idx,:], y[idx, 0]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "valid_dataset = CustomDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "462e8ff9-a61a-4702-be4e-e4994d94edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(max_nodes*n_prims, max_nodes*n_prims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max_nodes*n_prims, max_nodes*n_prims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max_nodes*n_prims, max_nodes*n_prims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max_nodes*n_prims, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29427a66-f5b7-4b61-9d53-a82495e685b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6892c6f-c0cd-4d24-90ef-0caa3b6b8fcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.141342  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n",
      "loss:     nan  [  501/  801]\n",
      "loss:     nan  [  601/  801]\n",
      "loss:     nan  [  701/  801]\n",
      "loss:     nan  [  801/  801]\n",
      "Test Error: Avg loss:      nan \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss:     nan  [    1/  801]\n",
      "loss:     nan  [  101/  801]\n",
      "loss:     nan  [  201/  801]\n",
      "loss:     nan  [  301/  801]\n",
      "loss:     nan  [  401/  801]\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "learning_rate = 1e-2\n",
    "batch_size = 1\n",
    "epochs = 1000\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(valid_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f8d54-2281-4794-a8b9-bc64fdac0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.zeros((max_nodes, n_prims))\n",
    "test[:6,:] = np.array([[1,0,0,0,0,0], [0,0,1,0,0,0], [0,0,0,0,0,1], [0,0,0,0,0,1], [0,0,0,0,1,0], [0,0,0,0,0,1]])\n",
    "test_tensor = torch.from_numpy(test.flatten())\n",
    "pred = model(test_tensor)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265cdad-3fc2-4d3f-ab9a-801cfd21e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    sm = torch.nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        x_reshaped = torch.reshape(x, (max_nodes, n_prims))\n",
    "    return sm(x_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910f330-e637-4c17-bcc7-3d47d739ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_tree(x0: np.array, learning_rate, max_iter):\n",
    "    x0 = torch.tensor(x0, requires_grad = True)\n",
    "    optimizer_tree = torch.optim.SGD([x0], lr = learning_rate)\n",
    "    #sm = torch.nn.Softmax(dim=1)\n",
    "    for i in range(max_iter):\n",
    "        pred = model(x0)\n",
    "        pred.backward()\n",
    "        optimizer_tree.step()\n",
    "        optimizer_tree.zero_grad()\n",
    "        #with torch.no_grad():\n",
    "        #    x0_reshaped = torch.reshape(x0, (max_nodes, n_prims))\n",
    "        #    x0 = sm(x0_reshaped).flatten().requires_grad_()\n",
    "        print(pred)\n",
    "        print(softmax(x0))\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e38260a-d5c4-4de0-b126-5f6e6880e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = generate_random_tree(pset, min_=1, max_=3)\n",
    "print(tree)\n",
    "x0 = tree_to_nodes_matrix(tree, pset, list(prims.keys()), n_nodes = max_nodes)\n",
    "x = optimize_tree(x0.ravel(), 1e-3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793ea46-01f6-421d-a581-2c81b4f1f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = torch.nn.Softmax(dim=1)\n",
    "with torch.no_grad():\n",
    "            x_reshaped = torch.reshape(x, (max_nodes, n_prims))\n",
    "            x = sm(x_reshaped)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88672cac-f4cf-4010-9943-1e448f3a02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(prims.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
